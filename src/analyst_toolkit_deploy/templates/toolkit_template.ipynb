{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "39b26b13",
            "metadata": {},
            "source": [
                "## üß™ Analyst Toolkit Tutorial: Full Data Pipeline\n",
                "\n",
                "This interactive notebook demonstrates the complete analyst pipeline using a synthetic **Palmer Penguins** dataset.\n",
                "\n",
                "Each step in the pipeline is modular, YAML-configurable, and produces exports, plots, and certification-ready reports.\n",
                "\n",
                "### üß∞ Toolkit Architecture: 3-Way Modular Design\n",
                "\n",
                "This pipeline is built around a flexible ETL framework with three usage modes:\n",
                "\n",
                "- üìì **Notebook Mode**: Run individual modules or the full pipeline interactively. Ideal for exploration and QA.\n",
                "- üßµ **CLI Mode**: Execute the full pipeline using `run_toolkit_pipeline.py`, controlled via a master YAML config.\n",
                "- üß™ **Hybrid Mode**: Develop in notebooks, deploy via scripts, reusing the same configs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "401b25f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìÅ 1. Load Configuration and Set Execution Context\n",
                "\n",
                "import logging\n",
                "import os\n",
                "from pathlib import Path\n",
                "from analyst_toolkit.m00_utils.config_loader import load_config\n",
                "from analyst_toolkit.m00_utils.load_data import load_csv\n",
                "from analyst_toolkit.m01_diagnostics.run_diag_pipeline import run_diag_pipeline\n",
                "from analyst_toolkit.m02_validation.run_validation_pipeline import run_validation_pipeline\n",
                "from analyst_toolkit.m03_normalization.run_normalization_pipeline import run_normalization_pipeline\n",
                "from analyst_toolkit.m04_duplicates.run_dupes_pipeline import run_duplicates_pipeline\n",
                "from analyst_toolkit.m05_detect_outliers.run_detection_pipeline import run_outlier_detection_pipeline\n",
                "from analyst_toolkit.m06_outlier_handling.run_handling_pipeline import run_outlier_handling_pipeline\n",
                "from analyst_toolkit.m07_imputation.run_imputation_pipeline import run_imputation_pipeline\n",
                "from analyst_toolkit.m10_final_audit.final_audit_pipeline import run_final_audit_pipeline\n",
                "\n",
                "# --- Find Project Root ---\n",
                "# This helper function makes the notebook runnable from any subdirectory\n",
                "# by locating the project root based on a set of marker directories.\n",
                "def find_project_root(markers=(\"config\", \"notebooks\", \"data\")):\n",
                "    \"\"\"Searches upward from the current directory for marker directories to find the project root.\"\"\"\n",
                "    current_path = Path.cwd().resolve()\n",
                "    for parent in [current_path, *current_path.parents]:\n",
                "        if all((parent / marker).is_dir() for marker in markers):\n",
                "            return parent\n",
                "    # Fallback to current working directory if no marker is found\n",
                "    print(f\"‚ö†Ô∏è Could not find project root with markers {markers}. Using current directory.\")\n",
                "    return Path.cwd()\n",
                "\n",
                "PROJECT_ROOT = find_project_root()\n",
                "print(f\"üìÇ Project Root detected: {PROJECT_ROOT}\")\n",
                "\n",
                "# --- Load Master Config ---\n",
                "# Path to master config, resolved from the project root for robustness\n",
                "master_config_path = PROJECT_ROOT / \"config\" / \"run_toolkit_config.yaml\"\n",
                "\n",
                "# Load master configuration dictionary\n",
                "master_config = load_config(master_config_path)\n",
                "\n",
                "# Extract run-level settings for use in all modules\n",
                "run_id = master_config.get(\"run_id\", \"default_run\")\n",
                "notebook_mode = master_config.get(\"notebook\", True)\n",
                "\n",
                "print(f\"üîß Config loaded | Run ID: {run_id} | Notebook Mode: {notebook_mode}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-data-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# üì• 2. Load Raw Data\n",
                "\n",
                "# Load input path from the master config\n",
                "input_path = master_config.get(\"pipeline_entry_path\")\n",
                "if not input_path:\n",
                "    raise ValueError(\"‚ùå 'pipeline_entry_path' not found in master config.\")\n",
                "\n",
                "print(f\"üìÇ Loading data from: {input_path}\")\n",
                "df_raw = load_csv(input_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b657ad0a",
            "metadata": {},
            "source": [
                "## üìä M01 ‚Äî Diagnostics\n",
                "\n",
                "This module generates a profile of the raw data: shape, types, nulls, skewness, and sample rows. It's the first step in understanding your dataset's structure and quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "diagnostics-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Run Diagnostics Module ---\n",
                "\n",
                "# Check if module is enabled in the master config\n",
                "module_settings = master_config.get(\"modules\", {}).get(\"diagnostics\", {})\n",
                "if not module_settings.get(\"run\", False):\n",
                "    print(\"‚è© Skipping Diagnostics module as per master config.\")\n",
                "else:\n",
                "    # Load module-specific config\n",
                "    diag_config_path = module_settings.get(\"config_path\", \"config/diag_config_template.yaml\")\n",
                "    diag_config = load_config(diag_config_path)\n",
                "    \n",
                "    print(f\"üöÄ Running Diagnostics from '{diag_config_path}'...\")\n",
                "    df_profiled = run_diag_pipeline(\n",
                "        config=diag_config,\n",
                "        df=df_raw,\n",
                "        notebook=notebook_mode,\n",
                "        run_id=run_id\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d1bab576",
            "metadata": {},
            "source": [
                "## üõ°Ô∏è M02 ‚Äî Validation (Audit Mode)\n",
                "\n",
                "This module audits the dataset against a defined schema to catch issues early and guide cleaning steps:\n",
                "\n",
                "- **Expected Columns & Dtypes**\n",
                "- **Allowed Categorical Values**\n",
                "- **Numeric Range Checks**\n",
                "\n",
                "In this first pass, `fail_on_error` is `false`, so it reports all issues without halting the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "validation-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Run Validation Module ---\n",
                "\n",
                "# Check if module is enabled in the master config\n",
                "module_settings = master_config.get(\"modules\", {}).get(\"validation\", {})\n",
                "if not module_settings.get(\"run\", False):\n",
                "    print(\"‚è© Skipping Validation module as per master config.\")\n",
                "else:\n",
                "    # Load module-specific config\n",
                "    config_path = module_settings.get(\"config_path\", \"config/validation_config_template.yaml\")\n",
                "    module_config = load_config(config_path)\n",
                "    \n",
                "    print(f\"üöÄ Running Validation from '{config_path}'...\")\n",
                "    # run_validation_pipeline returns (df, results_dict)\n",
                "    df_valid = run_validation_pipeline(\n",
                "        config=module_config,\n",
                "        df=df_profiled,\n",
                "        notebook=notebook_mode,\n",
                "        run_id=run_id\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b77d873b",
            "metadata": {},
            "source": [
                "## üßπ M03 ‚Äî Normalization\n",
                "\n",
                "This module performs rule-based cleaning and standardization to prepare the dataset for certification:\n",
                "\n",
                "- **Column Renaming & Type Coercion**\n",
                "- **Value Mapping & Text Cleaning**\n",
                "- **Fuzzy Matching & Datetime Parsing**\n",
                "\n",
                "All rules and output paths are controlled via the YAML config (`normalization_config_template.yaml`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "normalization-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Run Normalization Module ---\n",
                "\n",
                "# Check if module is enabled in the master config\n",
                "module_settings = master_config.get(\"modules\", {}).get(\"normalization\", {})\n",
                "if not module_settings.get(\"run\", False):\n",
                "    print(\"‚è© Skipping Normalization module as per master config.\")\n",
                "else:\n",
                "    # Load module-specific config\n",
                "    config_path = module_settings.get(\"config_path\", \"config/normalization_config_template.yaml\")\n",
                "    module_config = load_config(config_path)\n",
                "    \n",
                "    print(f\"üöÄ Running Normalization from '{config_path}'...\")\n",
                "    df_norm= run_normalization_pipeline(\n",
                "        config=module_config,\n",
                "        df=df_valid,\n",
                "        notebook=notebook_mode,\n",
                "        run_id=run_id\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "certification-gate-markdown",
            "metadata": {},
            "source": [
                "## üõ°Ô∏è M02 ‚Äî Certification Gate (Strict Mode)\n",
                "\n",
                "This step re-uses the **Validation Module (M02)**, but with a stricter configuration to act as a quality gate. It is designed to **halt the pipeline** if violations are found:\n",
                "\n",
                "- ‚úÖ All column names, data types, categorical values, and numeric ranges must pass\n",
                "- üõë **`fail_on_error: true`** triggers a hard stop on validation failure\n",
                "\n",
                "This step certifies the cleaned dataset before proceeding to more advanced steps like outlier handling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "certification-gate-code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Run Certification Gate (Strict Validation) ---\n",
                "\n",
                "# Check if module is enabled in the master config\n",
                "module_settings = master_config.get(\"modules\", {}).get(\"validation_gatekeeper\", {})\n",
                "if not module_settings.get(\"run\", False):\n",
                "    print(\"‚è© Skipping Certification Gate as per master config.\")\n",
                "else:\n",
                "    # Load module-specific config\n",
                "    config_path = module_settings.get(\"config_path\", \"config/certification_config_template.yaml\")\n",
                "    module_config = load_config(config_path)\n",
                "    \n",
                "    print(f\"üöÄ Running Certification Gate from '{config_path}'...\")\n",
                "    df_cert = run_validation_pipeline(\n",
                "        config=module_config,\n",
                "        df=df_norm,\n",
                "        notebook=notebook_mode,\n",
                "        run_id=run_id\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3e92304b",
            "metadata": {},
            "source": [
                "## ‚ôªÔ∏è M04 ‚Äî Deduplication\n",
                "\n",
                "This module identifies and handles **duplicate rows** in the dataset, using the logic from `m04_duplicates`.\n",
                "\n",
                "You can choose to:\n",
                "- üîç **Flag duplicates** for review (`mode: \"flag\"`)\n",
                "- ‚úÇÔ∏è **Remove duplicates** directly (`mode: \"remove\"`)\n",
                "\n",
                "The logic is configurable via `dups_config_template.yaml`, allowing you to specify which columns to check for duplication."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "60480b88",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Run Duplicates Module ---\n",
                "\n",
                "# Check if module is enabled in the master config\n",
                "module_settings = master_config.get(\"modules\", {}).get(\"duplicates\", {})\n",
                "if not module_settings.get(\"run\", False):\n",
                "    print(\"‚è© Skipping Duplicates module as per master config.\")\n",
                "else:\n",
                "    # Load module-specific config\n",
                "    config_path = module_settings.get(\"config_path\", \"config/dups_config_template.yaml\")\n",
                "    module_config = load_config(config_path)\n",
                "    \n",
                "    print(f\"üöÄ Running Duplicates from '{config_path}'...\")\n",
                "    # The function returns the dataframe and a results dictionary\n",
                "    df_duped= run_duplicates_pipeline(\n",
                "        config=module_config,\n",
                "        df=df_cert,\n",
                "        notebook=notebook_mode,\n",
                "        run_id=run_id\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "18d4bcd1",
            "metadata": {},
            "source": [
                "## üìè M05 ‚Äî Detect Outliers\n",
                "\n",
                "This module (`m05_detect_outliers`) scans numeric columns for outliers using configurable logic:\n",
                "\n",
                "- **Z-Score** or **IQR** methods (per column or global default)\n",
                "- Adds binary flags (e.g., `*_outlier`) to the dataset if `append_flags: true`\n",
                "- Skips non-numeric or excluded fields via `exclude_columns`\n",
                "\n",
                "üìä If enabled, an interactive **PlotViewer** renders boxplots, histograms, and violin plots inline, giving a fast visual summary of where anomalies occur."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8430fdd3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Detect Outliers ---\n",
                "\n",
                "# Initialize detection_results to ensure it exists for the next step\n",
                "detection_results = None\n",
                "\n",
                "# Check if module is enabled in the master config\n",
                "module_settings = master_config.get(\"modules\", {}).get(\"outlier_detection\", {})\n",
                "if not module_settings.get(\"run\", False):\n",
                "    print(\"‚è© Skipping Outlier Detection module as per master config.\")\n",
                "elif df is None:\n",
                "    print(\"‚è© Skipping Outlier Detection because input dataframe is None.\")\n",
                "else:\n",
                "    # Load module-specific config\n",
                "    config_path = module_settings.get(\"config_path\", \"config/outlier_config_template.yaml\")\n",
                "    module_config = load_config(config_path)\n",
                "    \n",
                "    print(f\"üöÄ Running Outlier Detection from '{config_path}'...\")\n",
                "    # This function returns (df_with_flags, detection_results_dict)\n",
                "    df_detect, detection_results = run_outlier_detection_pipeline(\n",
                "        config=module_config,\n",
                "        df=df_duped,\n",
                "        notebook=notebook_mode,\n",
                "        run_id=run_id\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "01fc91c8",
            "metadata": {},
            "source": [
                "## üßº M06 ‚Äî Handle Outliers\n",
                "\n",
                "This module (`m06_outlier_handling`) applies cleanup strategies to outliers flagged in the detection step:\n",
                "\n",
                "- **Strategies**: `clip` (cap to bounds), `median` (impute), `constant` (fill with a fixed value), or `none`.\n",
                "- **Configuration**: Apply rules globally (`__default__`) or per-column.\n",
                "\n",
                "This step is purely for remediation and relies on the `detection_results` from the previous module."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "17bb461c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Handle Outliers ---\n",
                "\n",
                "# Check if module is enabled in the master config\n",
                "module_settings = master_config.get(\"modules\", {}).get(\"outlier_handling\", {})\n",
                "if not module_settings.get(\"run\", False):\n",
                "    print(\"‚è© Skipping Outlier Handling module as per master config.\")\n",
                "elif df is None:\n",
                "    print(\"‚è© Skipping Outlier Handling because input dataframe is None.\")\n",
                "elif detection_results is None:\n",
                "    print(\"‚è© Skipping Outlier Handling because no detection results were provided from the previous step.\")\n",
                "else:\n",
                "    # Load module-specific config\n",
                "    config_path = module_settings.get(\"config_path\", \"config/handling_config_template.yaml\")\n",
                "    module_config = load_config(config_path)\n",
                "    \n",
                "    print(f\"üöÄ Running Outlier Handling from '{config_path}'...\")\n",
                "    # This function returns the dataframe with outliers handled\n",
                "    df_handled= run_outlier_handling_pipeline(\n",
                "        config=module_config,\n",
                "        df=df_detect,\n",
                "        detection_results=detection_results,  # Pass results from M05\n",
                "        notebook=notebook_mode,\n",
                "        run_id=run_id\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eaccca82",
            "metadata": {},
            "source": [
                "## üîß M07 ‚Äî Impute Missing Values\n",
                "\n",
                "This module (`m07_imputation`) fills missing (`NaN`) values using a column-specific strategy:\n",
                "\n",
                "- **Strategies**: `mean`, `median`, `mode`, or `constant`.\n",
                "- **Configuration**: Apply rules per column via `rules.strategies` in the YAML.\n",
                "\n",
                "üìä If enabled, comparison plots show how categorical columns changed post-imputation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a0fa72a6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Run Imputation Module ---\n",
                "\n",
                "# Check if module is enabled in the master config\n",
                "module_settings = master_config.get(\"modules\", {}).get(\"imputation\", {})\n",
                "if not module_settings.get(\"run\", False):\n",
                "    print(\"‚è© Skipping Imputation module as per master config.\")\n",
                "elif df is None:\n",
                "    print(\"‚è© Skipping Imputation because input dataframe is None.\")\n",
                "else:\n",
                "    # Load module-specific config\n",
                "    config_path = module_settings.get(\"config_path\", \"config/imputation_config_template.yaml\")\n",
                "    module_config = load_config(config_path)\n",
                "    \n",
                "    print(f\"üöÄ Running Imputation from '{config_path}'...\")\n",
                "    df_imput = run_imputation_pipeline(\n",
                "        config=module_config,\n",
                "        df=df_handled,\n",
                "        notebook=notebook_mode,\n",
                "        run_id=run_id\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "917560ac",
            "metadata": {},
            "source": [
                "## Alternative: Full Pipeline Runner\n",
                "\n",
                "For non-interactive runs, or to execute the entire pipeline in one go, you can use the `run_toolkit_pipeline` function from the `analyst_toolkit` library. This is particularly useful for automated scripts where step-by-step inspection is not required.\n",
                "\n",
                "To use it, you would import `run_toolkit_pipeline` and call it with the path to your master configuration file.\n",
                "\n",
                "```python\n",
                "# from analyst_toolkit.run_toolkit_pipeline import run_toolkit_pipeline\n",
                "#\n",
                "# This function runs all modules enabled in your 'run_toolkit_config.yaml' sequentially.\n",
                "# df_final, all_results = run_toolkit_pipeline(config_path=RUN_CONFIG_PATH)\n",
                "```\n",
                "\n",
                "> **Note:** This template notebook is designed for step-by-step execution and inspection. Using the full pipeline runner will execute all steps at once and bypass the individual cell outputs in this notebook."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fabd7e11",
            "metadata": {},
            "source": [
                "## üé¨ M10 ‚Äî Final Audit & Certification\n",
                "\n",
                "This final module (`m10_final_audit`) serves as the ultimate quality gate before exporting the cleaned dataset. It performs a comprehensive audit and applies strict certification checks.\n",
                "\n",
                "- ‚úÖ **Final Edits**: Drops or renames columns and coerces dtypes as needed.\n",
                "- ‚úÖ **Certification Check**: Re-runs validation rules with `fail_on_error: true` to enforce schema, dtypes, and content requirements.\n",
                "- ‚úÖ **Lifecycle Comparison**: Compares the raw vs. final dataset's structure, nulls, and column presence.\n",
                "- ‚úÖ **Capstone Report**: Renders a complete dashboard summarizing the pipeline's impact and status.\n",
                "\n",
                "üõ°Ô∏è If any rule is violated, the system halts and logs failure details for debugging. Once this step passes, your dataset is certified and ready for production use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3d790cfd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Run Final Audit Module ---\n",
                "\n",
                "# Check if module is enabled in the master config\n",
                "module_settings = master_config.get(\"modules\", {}).get(\"final_audit\", {})\n",
                "if not module_settings.get(\"run\", False):\n",
                "    print(\"‚è© Skipping Final Audit module as per master config.\")\n",
                "elif df is None:\n",
                "    print(\"‚è© Skipping Final Audit because input dataframe is None.\")\n",
                "else:\n",
                "    # Load module-specific config\n",
                "    config_path = module_settings.get(\"config_path\", \"config/final_audit_config_template.yaml\")\n",
                "    module_config = load_config(config_path)\n",
                "    \n",
                "    print(f\"üöÄ Running Final Audit from '{config_path}'...\")\n",
                "    # This function returns the final, certified dataframe\n",
                "    df_final = run_final_audit_pipeline(\n",
                "        config=module_config,\n",
                "        df=df_imput,\n",
                "        notebook=notebook_mode,\n",
                "        run_id=run_id\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "final-preview-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# üéâ Final Certified Data Preview\n",
                "if 'df' in locals() and df is not None:\n",
                "    print(\"‚úÖ Pipeline complete. Displaying the first 5 rows of the final certified dataset:\")\n",
                "    display(df.head())\n",
                "else:\n",
                "    print(\"‚èπÔ∏è Pipeline finished, but no final dataframe was produced (likely skipped or failed). \")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "dirty_birds_eda",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
